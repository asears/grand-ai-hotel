# Token Budgeting Strategies
*The Art of Context Management and Premium Request Optimization*

**Maintained by**: M. Gustave (Resource Allocation) and Ludwig (Precision Accounting)  
**Last Updated**: 2026-01-29

---

## ğŸ“Š Understanding Your Resources

### GitHub Copilot Premium Request Allocation

**Your Monthly Budget**:
- **2,000 premium requests per month** (Copilot Individual/Business)
- Resets on your billing cycle date
- Shared across all Copilot interfaces (Chat, CLI, Edits)
- Premium requests use advanced models (GPT-4, Claude Sonnet/Opus)

**What Counts as a Premium Request**:
- Each message in Copilot Chat using a premium model
- Each CLI command execution (`gh copilot suggest`, `gh copilot explain`)
- Each Copilot Edits operation
- Each inline completion request (minimal cost, but counted)

**What Doesn't Count**:
- Using standard/fast models when available
- Reading Copilot's responses
- Opening Copilot panels or windows
- Browsing Copilot documentation

---

## ğŸ¯ Token Fundamentals

### What Are Tokens?

**Definition**: Tokens are the atomic units of text that AI models process.

**Estimation Rules**:
- **~4 characters â‰ˆ 1 token** (English text average)
- **~0.75 words â‰ˆ 1 token** (English text average)
- Code is generally more token-dense than prose
- Special characters and formatting add tokens

**Examples**:
```
Text: "Hello, world!" 
Tokens: ~3 tokens

Text: "const fetchUser = async (id) => {...}"
Tokens: ~12 tokens

Text: Complete Python file (300 lines)
Tokens: ~2,000-3,000 tokens
```

### Context Windows by Model

| Model | Context Window | Typical Use | Cost Tier |
|-------|---------------|-------------|-----------|
| **GPT-4 Turbo** | ~128,000 tokens | Copilot Chat default | Premium |
| **GPT-4** | ~8,000 tokens | Older Copilot | Premium |
| **Claude Sonnet 4.5** | ~200,000 tokens | High-context tasks | Premium |
| **Claude Opus 4.5** | ~200,000 tokens | Complex reasoning | Ultra-premium |
| **Claude Haiku 4.5** | ~200,000 tokens | Fast operations | Standard |
| **GPT-3.5 Turbo** | ~16,000 tokens | Quick completions | Standard |

**Context Window Composition**:
```
Total Context Window = Input Tokens + Output Tokens

Input Tokens:
â”œâ”€ Your prompt
â”œâ”€ Open files in editor
â”œâ”€ Conversation history
â”œâ”€ System instructions
â””â”€ Selected code

Output Tokens:
â”œâ”€ Model's response
â””â”€ Generated code/text
```

---

## ğŸ’° Premium Request Budgeting

### Monthly Allocation Strategy

**The 2,000 Request Budget Breakdown**:

```
Total: 2,000 premium requests/month
â”‚
â”œâ”€ Daily Average: ~67 requests/day (30-day month)
â”‚   â””â”€ Hourly: ~8 requests/hour (8-hour workday)
â”‚
â”œâ”€ Recommended Allocation:
â”‚   â”œâ”€ Development Work: 60% (1,200 requests)
â”‚   â”œâ”€ Learning/Research: 20% (400 requests)
â”‚   â”œâ”€ Code Review: 10% (200 requests)
â”‚   â””â”€ Reserve/Overflow: 10% (200 requests)
â”‚
â””â”€ By Week:
    â”œâ”€ Week 1: 500 requests (buffer for complex work)
    â”œâ”€ Week 2: 500 requests
    â”œâ”€ Week 3: 500 requests
    â””â”€ Week 4: 500 requests (includes reserve)
```

### Request Cost Awareness

**Low-Cost Activities** (1-2 requests):
- Simple inline completions
- Quick questions with short answers
- File formatting
- Simple refactoring
- Documentation generation

**Medium-Cost Activities** (3-10 requests):
- Code generation for new features
- Debugging sessions
- Test generation
- Multi-file understanding
- Architecture discussions

**High-Cost Activities** (10-50+ requests):
- Complex refactoring across many files
- Deep debugging of production issues
- Learning new frameworks/languages
- Architectural redesigns
- Large-scale code reviews

**Very High-Cost Activities** (50-200+ requests):
- Multi-hour pair programming sessions
- Complete feature development
- System-wide refactoring
- Comprehensive code audits
- Migration projects

---

## ğŸ“ Token Estimation Strategies

### Estimating Your Prompt Cost

**Quick Estimation Formula**:
```
Prompt Tokens â‰ˆ (Characters Ã· 4) + (Open Files Ã— 1000) + (Conversation History Ã— 500)
```

**Example Calculation**:
```
Scenario: Ask Copilot to review a function

Your prompt: 200 characters â†’ ~50 tokens
Selected code: 800 characters â†’ ~200 tokens  
Open file context: 2 files â†’ ~2,000 tokens
Conversation history: 5 turns â†’ ~2,500 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Input: ~4,750 tokens

Expected output: ~500-1,000 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Context Used: ~5,250-5,750 tokens
```

### Tools for Token Counting

**Manual Estimation**:
```python
# Quick Python token estimator
def estimate_tokens(text: str) -> int:
    """Rough token estimation for English text and code."""
    return len(text) // 4

# Usage
code = open('myfile.py').read()
estimated = estimate_tokens(code)
print(f"Estimated tokens: {estimated}")
```

**Online Tools**:
- OpenAI Tokenizer: https://platform.openai.com/tokenizer
- Anthropic Token Counter: Built into Claude interface
- tiktoken (Python library): `pip install tiktoken`

**VS Code Extension**:
```
Extension: "Token Counter"
- Shows token count in status bar
- Estimates cost per request
- Tracks daily usage
```

---

## âš¡ Model Tier Selection

### Choosing the Right Model for the Task

#### Use Standard/Fast Models (Haiku, GPT-3.5) When:

**Perfect For**:
- âœ… Code formatting and linting
- âœ… Simple refactoring (variable renames, import organization)
- âœ… Documentation generation
- âœ… Translating code between similar languages
- âœ… Generating boilerplate code
- âœ… Quick factual questions
- âœ… Syntax corrections
- âœ… Unit test generation for simple functions

**Example Prompts**:
```
"Format this Python file with black"
"Add docstrings to these functions"
"Generate Jest tests for this utility function"
"Rename variable 'x' to 'userId' throughout this file"
```

**Cost**: Typically FREE or minimal premium request usage

---

#### Use Premium Models (Sonnet, GPT-4) When:

**Perfect For**:
- âœ… Complex code generation
- âœ… Architectural decisions
- âœ… Debugging tricky issues
- âœ… Multi-file refactoring
- âœ… Learning new concepts
- âœ… Code reviews requiring nuance
- âœ… Security analysis
- âœ… Performance optimization

**Example Prompts**:
```
"Design a microservices architecture for this monolith"
"Debug why this async function causes a race condition"
"Refactor this class to use dependency injection"
"Review this authentication code for security vulnerabilities"
```

**Cost**: 1 premium request per interaction

---

#### Use Ultra-Premium Models (Opus) When:

**Perfect For**:
- âœ… Novel algorithmic challenges
- âœ… Complex system design
- âœ… Security audits of critical systems
- âœ… Performance optimization requiring deep analysis
- âœ… Solving truly hard problems
- âœ… Creative problem-solving
- âœ… Research-level questions

**Example Prompts**:
```
"Design a custom consensus algorithm for our distributed system"
"Optimize this compiler pass for LLVM"
"Audit this cryptographic implementation"
"Find the optimal data structure for this constraint satisfaction problem"
```

**Cost**: 1 premium request (higher token cost per request)

---

### Model Selection Decision Tree

```
Is the task routine/mechanical?
â”œâ”€ YES â†’ Use Fast Model (Haiku/GPT-3.5)
â”‚         Examples: formatting, simple refactoring
â”‚
â””â”€ NO â†’ Is the task complex but standard?
        â”œâ”€ YES â†’ Use Premium Model (Sonnet/GPT-4)
        â”‚         Examples: feature development, debugging
        â”‚
        â””â”€ NO â†’ Is the task exceptionally difficult?
                â”œâ”€ YES â†’ Use Ultra-Premium (Opus)
                â”‚         Examples: novel algorithms, research
                â”‚
                â””â”€ UNSURE â†’ Start with Sonnet, escalate if needed
```

---

## ğŸ§  Context Window Monitoring

### Understanding Context Saturation

**Context Window Filling Process**:
```
Start of Conversation: 0% full
â”œâ”€ Turn 1: Prompt + Response â†’ 5% full
â”œâ”€ Turn 10: â†’ 25% full  
â”œâ”€ Turn 30: â†’ 60% full
â”œâ”€ Turn 50: â†’ 85% full (performance degradation begins)
â””â”€ Turn 70: â†’ 95% full (compaction recommended)
```

**Signs You're Approaching Limit**:
- âš ï¸ Slower response times
- âš ï¸ Model seems to "forget" earlier conversation
- âš ï¸ Responses become less accurate
- âš ï¸ Warnings about context window
- âš ï¸ Conversation exceeds 50+ turns

### Checking Context Usage

**In Copilot Chat**:
```
Unfortunately, Copilot doesn't show token count directly.
Monitor by conversation length and complexity.

Rule of thumb:
- Short conversation (< 10 turns): ~10-20% context
- Medium conversation (10-30 turns): ~30-60% context  
- Long conversation (30-50 turns): ~60-85% context
- Very long (50+ turns): ~85-95%+ context
```

**In Claude (Grand Budapest Agents)**:
```
Command: /context

Output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Window Usage               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total: 67,234 / 200,000 tokens     â”‚
â”‚ Usage: 34%                         â”‚
â”‚ Turns: 42                          â”‚
â”‚ Status: Healthy                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Context Window Optimization

**Reduce Context Consumption**:

1. **Close Unnecessary Files**
   ```
   Each open file adds ~500-2,000 tokens
   Close files not relevant to current task
   ```

2. **Clear Conversation History**
   ```
   Start new chat for unrelated topics
   Don't carry over irrelevant context
   ```

3. **Be Concise in Prompts**
   ```
   âŒ "I was wondering if you might be able to help me with something.
      I'm working on this project and I have this function that I wrote
      but I'm not sure if it's the best way to do it..."
      
   âœ… "Review this function for performance issues: [code]"
   ```

4. **Use File References Instead of Pasting**
   ```
   âŒ [Paste entire 500-line file]
      "Review this file"
      
   âœ… "Review src/utils/auth.ts" (if file is already open)
   ```

5. **Selective Code Selection**
   ```
   Select only relevant functions/classes
   Don't include entire files unless necessary
   ```

---

## ğŸ”„ Strategic Compaction Timing

### What is Compaction?

**Definition**: Compaction summarizes conversation history to free context window space while preserving important information.

**What Happens During Compaction**:
```
Before Compaction:
â”œâ”€ All conversation turns (1-50+)
â”œâ”€ All code snippets
â”œâ”€ All explanations
â””â”€ Total: 150,000 tokens

After Compaction:
â”œâ”€ Summary of earlier conversation
â”œâ”€ Recent turns (last ~15-20) preserved verbatim
â”œâ”€ Key decisions documented
â”œâ”€ Current task context maintained
â””â”€ Total: ~40,000 tokens
```

### When to Compact (Strategic Timing)

**âœ… Good Times to Compact**:

1. **After Major Milestone**
   ```
   "We just completed the API implementation.
    Let's compact before starting the frontend."
   ```

2. **Before Topic Switch**
   ```
   "Finished debugging. Now moving to performance optimization.
    Compacting to start fresh..."
   ```

3. **At Natural Breakpoints**
   ```
   - End of work session
   - Lunch breaks
   - After code review
   - Before major refactoring
   ```

4. **When Warned**
   ```
   If model says: "Context window is approaching limit"
   â†’ Compact immediately
   ```

5. **Proactive Compaction (50+ turns)**
   ```
   Even if not warned, compact at ~50 turns
   to maintain performance
   ```

**âŒ Bad Times to Compact**:

1. **During Active Debugging**
   ```
   You're tracing through stack traces and error messages
   â†’ Don't compact, you'll lose critical context
   ```

2. **Mid-Implementation**
   ```
   You're iterating on a complex function
   â†’ Wait until implementation is complete
   ```

3. **During Code Review**
   ```
   You're going through multiple files for review
   â†’ Compact after review is complete
   ```

4. **Before Getting Final Answer**
   ```
   You asked a complex question
   â†’ Wait for complete answer before compacting
   ```

5. **When Model Needs Recent Details**
   ```
   "Using the types we defined 3 turns ago..."
   â†’ Those details would be summarized away
   ```

### Compaction Best Practices

**Before Compacting**:
```
1. Save important artifacts to files
   - Design decisions â†’ docs/architecture.md
   - Code snippets â†’ src/
   - Test cases â†’ tests/

2. Summarize key points yourself
   - What was decided
   - What was implemented  
   - What's next

3. Ensure current work is saved
   - Commit code to git
   - Save open files
```

**Compaction Commands**:

**In Claude**:
```
/compact [optional: what to preserve]

Examples:
/compact
/compact preserve the API design we discussed
/compact keep the security requirements
```

**In Copilot Chat**:
```
Copilot doesn't have explicit compaction.
Instead:
- Start a new chat for new topics
- Reference previous work explicitly
- Save artifacts to workspace files
```

**After Compacting**:
```
1. Verify critical context survived
   - Ask: "What were we working on?"
   - Check: Does model remember key decisions?

2. Reference saved artifacts
   - "Continuing from architecture.md..."
   - "Implementing the design we documented..."

3. Start fresh if context is lost
   - Load relevant files
   - Restate current goal
   - Reference documentation
```

---

## ğŸ“… Multi-Session Planning

### Long-Term Project Strategies

**Project Duration > 1 Day**:
Use multiple sessions with context transfer.

**Session 1: Planning & Design** (Premium)
```
Duration: 1-2 hours
Model: Sonnet or Opus
Tasks:
â”œâ”€ Requirements gathering
â”œâ”€ Architecture design  
â”œâ”€ Technology selection
â””â”€ Create design docs

Output Artifacts:
â”œâ”€ docs/architecture.md
â”œâ”€ docs/requirements.md
â””â”€ docs/tech-stack.md

Token Budget: ~300-500 requests
```

**Session 2-N: Implementation** (Mixed)
```
Duration: Multiple sessions
Models: Mostly Haiku/Standard, Sonnet when needed
Tasks:
â”œâ”€ Implement features
â”œâ”€ Write tests
â”œâ”€ Iterate on feedback
â””â”€ Document as you go

Output Artifacts:
â”œâ”€ src/ (code)
â”œâ”€ tests/ (tests)
â””â”€ README updates

Token Budget: ~100-200 requests per session
Strategy: Use fast models for routine work
```

**Final Session: Review & Polish** (Premium)
```
Duration: 1-2 hours
Model: Sonnet
Tasks:
â”œâ”€ Code review
â”œâ”€ Security audit
â”œâ”€ Performance check
â””â”€ Documentation review

Token Budget: ~200-300 requests
```

### Context Transfer Between Sessions

**Method 1: Workspace Files**
```
Session 1: Create docs/session-notes.md
â”œâ”€ Document decisions
â”œâ”€ List completed tasks
â”œâ”€ Note blockers
â””â”€ Define next steps

Session 2: Reference session-notes.md
â”œâ”€ "Continuing from session-notes.md..."
â”œâ”€ "Implementing the API design from docs/architecture.md..."
â””â”€ Update session-notes.md with progress
```

**Method 2: Git Commits**
```
Session 1:
â”œâ”€ Implement feature
â”œâ”€ Commit with detailed message
â””â”€ End session

Session 2:
â”œâ”€ "Review my last commit"
â”œâ”€ "Continue implementing feature X from last commit"
â””â”€ Context automatically loaded from git history
```

**Method 3: Explicit Summaries**
```
End of Session 1:
"Summarize what we accomplished today and what's next"
â†’ Copy summary to clipboard

Start of Session 2:
"Here's where we left off: [paste summary]
 Let's continue..."
```

---

## ğŸ’¡ Cost Optimization Techniques

### Technique 1: Batch Similar Tasks

**Inefficient** (Multiple requests):
```
Request 1: "Add docstring to function A"
Request 2: "Add docstring to function B"  
Request 3: "Add docstring to function C"
Request 4: "Add docstring to function D"

Cost: 4 premium requests
```

**Efficient** (Single request):
```
Request: "Add docstrings to functions A, B, C, and D in this file"

Cost: 1 premium request
```

**Savings**: 75% reduction in requests

---

### Technique 2: Use CLI for Commands

**Inefficient**:
```
Copilot Chat: "How do I find all TODO comments in my codebase?"
â†’ Wait for explanation
â†’ Copy command
â†’ Run in terminal

Cost: 1 premium request + time
```

**Efficient**:
```
gh copilot suggest "find all TODO comments in git-tracked files"
â†’ Get command immediately
â†’ Execute

Cost: 1 CLI request (faster, optimized for commands)
```

---

### Technique 3: Incremental Prompts

**Inefficient** (All at once):
```
"Create a REST API with authentication, rate limiting, caching,
 logging, error handling, tests, documentation, and deployment config"

Result: Overwhelming response, may miss requirements
Cost: 1 request, but likely needs iteration (3-5 total)
```

**Efficient** (Incremental):
```
Request 1: "Create basic REST API structure"
Request 2: "Add JWT authentication"
Request 3: "Add rate limiting middleware"
Request 4: "Add request logging"
...

Result: Better control, fewer iterations
Cost: 5-7 requests, but more precise
```

**When to Use Which**:
- Use all-at-once for simple tasks
- Use incremental for complex tasks or learning

---

### Technique 4: Self-Service First

**Before Asking Copilot**:
```
1. Check documentation
2. Search codebase with grep/find
3. Read error messages carefully
4. Try obvious solution
5. If stuck > 15 minutes â†’ Ask Copilot
```

**This Saves**:
- Premium requests for truly complex issues
- Develops your problem-solving skills
- Faster resolution for simple issues

**When to Ask Immediately**:
- Unfamiliar technology
- Complex architectural decisions
- Security-sensitive code
- Time-critical debugging

---

### Technique 5: Model Switching

**Start with Standard â†’ Escalate if Needed**:
```
Attempt 1: Fast model (free/cheap)
â”œâ”€ "Format this code"
â””â”€ If successful: Done âœ“

Attempt 2: Premium model
â”œâ”€ "Refactor this for better performance"
â””â”€ If not satisfactory: Escalate

Attempt 3: Ultra-premium model
â”œâ”€ "Design optimal algorithm for this constraint"
â””â”€ Complex reasoning required
```

**Don't**:
```
âŒ Always use Opus for everything (waste)
âŒ Always use Haiku for everything (inadequate for complex tasks)
```

**Do**:
```
âœ… Match model to task complexity
âœ… Start cheap, escalate when needed
âœ… Use Opus sparingly for truly hard problems
```

---

### Technique 6: Workspace Organization

**Efficient Context Loading**:
```
Organized Workspace:
â”œâ”€ Open only files relevant to current task
â”œâ”€ Close files when done with them
â”œâ”€ Use workspaces to group related files
â””â”€ Clear file structure

Result: Lower token consumption per request
```

**Inefficient Context Loading**:
```
Messy Workspace:
â”œâ”€ 20+ files open  
â”œâ”€ Unrelated files mixed together
â”œâ”€ Large files open unnecessarily
â””â”€ Poor organization

Result: Every request includes unnecessary context
```

**Token Savings**: 30-50% per request

---

### Technique 7: Prompt Templates

**Create Reusable Templates**:

```markdown
<!-- template-code-review.md -->
Review this [LANGUAGE] code for:
1. Security vulnerabilities
2. Performance issues
3. Code quality and maintainability
4. Best practices compliance

Code:
[CODE_HERE]

Provide specific line numbers and actionable suggestions.
```

**Usage**:
```
Copy template â†’ Fill in blanks â†’ Send
Consistent, efficient, no wasted tokens on rephrasing
```

**Common Templates**:
- Code review
- Test generation
- Bug report
- Feature request
- Refactoring request
- Documentation generation

---

## ğŸ“Š Usage Tracking & Monitoring

### Manual Tracking

**Spreadsheet Method**:
```
Date       | Task              | Requests | Model  | Notes
-----------|-------------------|----------|--------|------------------
2024-01-29 | Feature dev       | 47       | Sonnet | New API endpoint
2024-01-29 | Code review       | 12       | Sonnet | Security audit
2024-01-29 | Quick fixes       | 8        | Haiku  | Formatting
-----------|-------------------|----------|--------|------------------
Daily:     |                   | 67       |        |
Monthly:   |                   | 1,243    |        | 757 remaining
```

### Automated Tracking

**Script to Track Copilot Usage** (Conceptual):
```bash
# Note: GitHub doesn't provide official API for this yet
# This is a manual logging approach

# Create log file
LOG_FILE="$HOME/.copilot-usage.log"

# Add alias to track usage
alias copilot-log='echo "$(date +%Y-%m-%d) - $1" >> $LOG_FILE'

# Usage
copilot-log "Code review session - ~20 requests"

# View monthly total
grep "$(date +%Y-%m)" $LOG_FILE | wc -l
```

### Budget Alerts

**Set Up Reminders**:
```
Week 1: Check usage (should be ~500)
Week 2: Check usage (should be ~1,000 total)
Week 3: Check usage (should be ~1,500 total)  
Week 4: Conserve if > 1,800

If > 1,600 by Week 3:
â””â”€ Switch to standard models for routine tasks
```

---

## ğŸ¯ Advanced Optimization Strategies

### Strategy 1: The Triage System

**Before Each Request, Ask**:

```
Urgency:
â”œâ”€ Critical (blocking work) â†’ Use premium
â”œâ”€ Important (needed soon) â†’ Use premium
â”œâ”€ Nice-to-have â†’ Use standard or defer
â””â”€ Learning â†’ Use premium strategically

Complexity:
â”œâ”€ Trivial â†’ Use standard model
â”œâ”€ Moderate â†’ Use standard, escalate if needed
â”œâ”€ Complex â†’ Use premium
â””â”€ Very complex â†’ Use ultra-premium

Alternatives:
â”œâ”€ Can I find this in docs? â†’ Read docs first
â”œâ”€ Can I experiment myself? â†’ Try first
â”œâ”€ Do I need AI for this? â†’ Consider manual approach
â””â”€ Is this best use of premium request? â†’ Triage
```

---

### Strategy 2: The Learning Investment

**Invest in Learning Early**:
```
Week 1: Heavy usage learning new framework
â”œâ”€ Use premium models liberally
â”œâ”€ Ask lots of questions
â”œâ”€ Build strong foundation
â””â”€ Cost: 600-800 requests

Weeks 2-4: Reap benefits
â”œâ”€ Reduced questions needed
â”œâ”€ Can solve more independently
â”œâ”€ Use standard models for routine work
â””â”€ Cost: 300-400 requests/week
```

**ROI**: Higher upfront cost, but overall efficiency gain

---

### Strategy 3: The Code Review Partnership

**Hybrid Human + AI Review**:
```
First Pass (You): Manual review
â”œâ”€ Obvious issues
â”œâ”€ Style violations
â”œâ”€ Simple bugs
â””â”€ Cost: 0 requests

Second Pass (AI): Deep review
â”œâ”€ Security vulnerabilities
â”œâ”€ Performance issues
â”œâ”€ Edge cases
â”œâ”€ Architecture concerns
â””â”€ Cost: 1-2 requests (targeted)

Result: Better reviews, lower cost
```

---

### Strategy 4: The Documentation Buffer

**Maintain Living Documentation**:
```
docs/
â”œâ”€ architecture.md (agent-generated, reviewed)
â”œâ”€ coding-standards.md (one-time generation)
â”œâ”€ common-patterns.md (accumulated over time)
â””â”€ faq.md (built up from Q&A sessions)

When Questions Arise:
1. Check documentation first
2. If not found, ask Copilot
3. Add answer to documentation
4. Reuse documentation in future

Token Savings: 
- Initial: High cost to create docs
- Ongoing: Massive savings (reference instead of asking)
```

---

### Strategy 5: The Pair Programming Protocol

**Efficient Long Sessions**:
```
Hour 1: High-Touch (Premium Model)
â”œâ”€ Architecture discussion
â”œâ”€ Complex problem solving
â”œâ”€ Design decisions
â””â”€ Cost: ~50-80 requests

Hour 2-3: Implementation (Mixed Models)
â”œâ”€ Use standard for boilerplate  
â”œâ”€ Use premium for complex logic
â”œâ”€ Reference decisions from Hour 1
â””â”€ Cost: ~30-50 requests/hour

Hour 4: Review (Premium Model)
â”œâ”€ Code review
â”œâ”€ Refinement
â”œâ”€ Documentation
â””â”€ Cost: ~30-40 requests

Total: ~140-220 requests for 4-hour session
```

---

## ğŸ† Token Efficiency Hall of Fame

### Champion Strategies from Grand Budapest Staff

**M. Gustave's Rule**:
> "One should never request what one already possesses in documentation."

**Application**:
- Maintain project documentation
- Reference existing patterns
- Build on previous work
- Save ~200 requests/month

---

**Zero's Speed Trick**:
> "For the hundredth formatting request, create a script."

**Application**:
```bash
# Instead of asking Copilot each time:
alias format-py="ruff format . && ruff check --fix ."
alias format-ts="prettier --write 'src/**/*.ts'"

# Savings: ~50-100 requests/month
```

---

**Ludwig's Type System Philosophy**:
> "Types documented once save questions asked a thousand times."

**Application**:
- Invest in strong type definitions
- Use TypeScript/Rust/typed languages
- Types answer questions automatically
- Save ~100 requests/month on "what type is this?"

---

**Agatha's Testing Wisdom**:
> "Tests written with care prevent debugging despair."

**Application**:
- Use premium requests for quality test generation
- Comprehensive tests reduce debugging needs
- Net savings from fewer debugging sessions
- Save ~150 requests/month on debugging

---

**Dmitri's Efficiency Mandate**:
> "Never use premium when standard suffices."

**Application**:
- Default to standard models
- Escalate only when necessary
- Save premium for complex tasks
- Save ~300 requests/month

---

**Henckels' Automation Doctrine**:
> "Automate repetition, reserve AI for cognition."

**Application**:
- Create scripts for repeated tasks
- Use CI/CD for mechanical work
- Save AI for decisions
- Save ~250 requests/month

---

## ğŸ’¡ Quick Tips

### Request Conservation
- **Batch similar tasks** into single requests
- **Use CLI** for command generation (more efficient)
- **Check docs first** before asking questions
- **Start with standard models**, escalate if needed
- **Close unused files** to reduce context size

### Context Management
- **Monitor conversation length** (compact at ~50 turns)
- **Start new chats** for unrelated topics
- **Save artifacts to files** before compacting
- **Reference documentation** instead of re-asking
- **Use selective code selection**, not whole files

### Model Selection
- **Haiku/GPT-3.5**: Formatting, simple refactoring, quick questions
- **Sonnet/GPT-4**: Feature development, debugging, reviews
- **Opus**: Novel algorithms, very complex problems only
- **Match model to task complexity** for efficiency

### Long-Term Projects
- **Plan multi-session workflow** at the start
- **Document decisions** in workspace files
- **Use git commits** for context transfer
- **Create templates** for common requests
- **Build documentation** to reduce future questions

### Budget Tracking
- **Check usage weekly** (target ~500/week)
- **Conserve in Week 4** if approaching limit
- **Invest heavily in learning** (pays off later)
- **Automate repetitive tasks** to save requests
- **Review usage patterns** monthly for optimization

---

## ğŸ”— Related Documentation

- [Claude Best Practices](./claude-best-practices.md) - Context window management
- [Copilot Best Practices](./copilot-best-practices.md) - Effective prompting
- [Agent Collaboration Guide](./agent-collaboration.md) - Multi-agent efficiency
- [Python Coding Standards](./python-standards.md) - Code quality

---

## ğŸ“š Tutorials Referenced

- **The Memory Palace** - Advanced context management techniques
- **The Concierge's Budget** - Resource allocation strategies  
- **The Accountant's Ledger** - Usage tracking methods
- **The Efficiency Expert** - Optimization patterns
- **The Long Game** - Multi-session planning

---

## ğŸ“ˆ Monthly Review Template

```markdown
## Month: [Month Year]

### Usage Statistics
- Total Requests Used: ___ / 2,000
- Average per Day: ___
- Peak Day: ___ requests
- Quietest Day: ___ requests

### Breakdown by Activity
- Development: ___ requests (___%)
- Code Review: ___ requests (___%)
- Learning: ___ requests (___%)
- Debugging: ___ requests (___%)
- Other: ___ requests (___%)

### Model Distribution
- Standard (Haiku/GPT-3.5): ___ requests
- Premium (Sonnet/GPT-4): ___ requests
- Ultra-Premium (Opus): ___ requests

### Efficiency Metrics
- Average Requests per Feature: ___
- Requests Saved by Documentation: ___
- Requests Saved by Automation: ___

### Optimization Opportunities
1. [What could be automated?]
2. [What could be documented?]
3. [Where did we overuse premium models?]
4. [Where did we underuse AI assistance?]

### Next Month Goals
- Target Usage: ___ requests
- Focus Areas: ___
- Automation Goals: ___
- Learning Investments: ___
```

---

**"Elegance in resource management is not about restriction, but about intentional allocation toward maximum value."** â€” M. Gustave H.

*For questions about token budgeting and optimization, consult Ludwig at the accounting desk.*
