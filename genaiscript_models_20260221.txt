GenAIScript
This is the current model alias mapping:

--- > claude-opus-4.6, Claude Opus 4.6, max 127805t

--- > claude-sonnet-4.6, Claude Sonnet 4.6, max 127805t

--- > gemini-3.1-pro-preview, Gemini 3.1 Pro (Preview), max 108609t

--- > gpt-5.2-codex, GPT-5.2-Codex, max 271805t

--- > gpt-5.3-codex, GPT-5.3-Codex, max 271805t

--- > gpt-5-mini, GPT-5 mini, max 127805t

--- > gpt-4o-mini, GPT-4o mini, max 12093t

--- > copilot-fast, GPT-4o mini, max 12093t

--- > gpt-4o, GPT-4o, max 123712t

--- > grok-code-fast-1, Grok Code Fast 1, max 108609t

--- > gpt-5.1, GPT-5.1, max 127805t

--- > gpt-5.1-codex, GPT-5.1-Codex, max 127805t

--- > gpt-5.1-codex-mini, GPT-5.1-Codex-Mini (Preview), max 127805t

--- > gpt-5.1-codex-max, GPT-5.1-Codex-Max, max 127805t

--- > claude-sonnet-4, Claude Sonnet 4, max 127805t

--- > claude-sonnet-4.5, Claude Sonnet 4.5, max 127805t

--- > claude-opus-4.5, Claude Opus 4.5, max 127805t

--- > claude-haiku-4.5, Claude Haiku 4.5, max 127805t

--- > gemini-3-pro-preview, Gemini 3 Pro (Preview), max 108609t

--- > gemini-3-flash-preview, Gemini 3 Flash (Preview), max 108609t

--- > gemini-2.5-pro, Gemini 2.5 Pro, max 108609t

--- > gpt-4.1, GPT-4.1, max 111424t

--- > oswe-vscode-prime, Raptor mini (Preview), max 199805t

--- > gpt-5.2, GPT-5.2, max 127805t

--- > auto, Auto, max 127805t

--- > claude-sonnet-4.6, Claude Sonnet 4.6, max 128000t

--- > claude-sonnet-4.5, Claude Sonnet 4.5, max 128000t

--- > claude-haiku-4.5, Claude Haiku 4.5, max 128000t

--- > claude-opus-4.6, Claude Opus 4.6, max 128000t

--- > claude-opus-4.6-fast, Claude Opus 4.6 (fast mode), max 128000t

--- > claude-opus-4.5, Claude Opus 4.5, max 128000t

--- > claude-sonnet-4, Claude Sonnet 4, max 128000t

--- > gemini-3-pro-preview, Gemini 3 Pro (Preview), max 128000t

--- > gpt-5.3-codex, GPT-5.3-Codex, max 272000t

--- > gpt-5.2-codex, GPT-5.2-Codex, max 272000t

--- > gpt-5.2, GPT-5.2, max 128000t

--- > gpt-5.1-codex-max, GPT-5.1-Codex-Max, max 128000t

--- > gpt-5.1-codex, GPT-5.1-Codex, max 128000t

--- > gpt-5.1, GPT-5.1, max 128000t

--- > gpt-5.1-codex-mini, GPT-5.1-Codex-Mini, max 128000t

--- > gpt-5-mini, GPT-5 mini, max 128000t

--- > gpt-4.1, GPT-4.1, max 64000t

--- > Qwen/Qwen3.5-397B-A17B:cheapest, Qwen/Qwen3.5-397B-A17B (cheapest), max 246144t

--- > Qwen/Qwen3.5-397B-A17B:fastest, Qwen/Qwen3.5-397B-A17B (fastest), max 246144t

--- > Qwen/Qwen3.5-397B-A17B:novita, Qwen/Qwen3.5-397B-A17B via novita, max 246144t

--- > Qwen/Qwen3.5-397B-A17B:together, Qwen/Qwen3.5-397B-A17B via together, max 246144t

--- > zai-org/GLM-5:cheapest, zai-org/GLM-5 (cheapest), max 186752t

--- > zai-org/GLM-5:fastest, zai-org/GLM-5 (fastest), max 186752t

--- > zai-org/GLM-5:together, zai-org/GLM-5 via together, max 186752t

--- > MiniMaxAI/MiniMax-M2.5:cheapest, MiniMaxAI/MiniMax-M2.5 (cheapest), max 188800t

--- > MiniMaxAI/MiniMax-M2.5:fastest, MiniMaxAI/MiniMax-M2.5 (fastest), max 188800t

--- > MiniMaxAI/MiniMax-M2.5:novita, MiniMaxAI/MiniMax-M2.5 via novita, max 188800t

--- > moonshotai/Kimi-K2.5:cheapest, moonshotai/Kimi-K2.5 (cheapest), max 246144t

--- > moonshotai/Kimi-K2.5:fastest, moonshotai/Kimi-K2.5 (fastest), max 246144t

--- > moonshotai/Kimi-K2.5:novita, moonshotai/Kimi-K2.5 via novita, max 246144t

--- > moonshotai/Kimi-K2.5:together, moonshotai/Kimi-K2.5 via together, max 246144t

--- > moonshotai/Kimi-K2.5:fireworks-ai, moonshotai/Kimi-K2.5 via fireworks-ai, max 246144t

--- > Qwen/Qwen3-Coder-Next:cheapest, Qwen/Qwen3-Coder-Next (cheapest), max 246144t

--- > Qwen/Qwen3-Coder-Next:fastest, Qwen/Qwen3-Coder-Next (fastest), max 246144t

--- > Qwen/Qwen3-Coder-Next:novita, Qwen/Qwen3-Coder-Next via novita, max 246144t

--- > zai-org/GLM-4.7-Flash:cheapest, zai-org/GLM-4.7-Flash (cheapest), max 184000t

--- > zai-org/GLM-4.7-Flash:fastest, zai-org/GLM-4.7-Flash (fastest), max 184000t

--- > zai-org/GLM-4.7-Flash:novita, zai-org/GLM-4.7-Flash via novita, max 184000t

--- > zai-org/GLM-4.7-Flash:zai-org, zai-org/GLM-4.7-Flash via zai-org, max 112000t

--- > openai/gpt-oss-20b:cheapest, openai/gpt-oss-20b (cheapest), max 115072t

--- > openai/gpt-oss-20b:fastest, openai/gpt-oss-20b (fastest), max 115072t

--- > openai/gpt-oss-20b:groq, openai/gpt-oss-20b via groq, max 115072t

--- > openai/gpt-oss-20b:nscale, openai/gpt-oss-20b via nscale, max 115072t

--- > openai/gpt-oss-20b:hyperbolic, openai/gpt-oss-20b via hyperbolic, max 115072t

--- > openai/gpt-oss-20b:together, openai/gpt-oss-20b via together, max 115072t

--- > openai/gpt-oss-20b:fireworks-ai, openai/gpt-oss-20b via fireworks-ai, max 115072t

--- > openai/gpt-oss-20b:ovhcloud, openai/gpt-oss-20b via ovhcloud, max 115072t

--- > openai/gpt-oss-120b:cheapest, openai/gpt-oss-120b (cheapest), max 115072t

--- > openai/gpt-oss-120b:fastest, openai/gpt-oss-120b (fastest), max 115072t

--- > openai/gpt-oss-120b:groq, openai/gpt-oss-120b via groq, max 115072t

--- > openai/gpt-oss-120b:novita, openai/gpt-oss-120b via novita, max 115072t

--- > openai/gpt-oss-120b:cerebras, openai/gpt-oss-120b via cerebras, max 112000t

--- > openai/gpt-oss-120b:nscale, openai/gpt-oss-120b via nscale, max 115072t

--- > openai/gpt-oss-120b:hyperbolic, openai/gpt-oss-120b via hyperbolic, max 115072t

--- > openai/gpt-oss-120b:together, openai/gpt-oss-120b via together, max 115072t

--- > openai/gpt-oss-120b:fireworks-ai, openai/gpt-oss-120b via fireworks-ai, max 115072t

--- > openai/gpt-oss-120b:scaleway, openai/gpt-oss-120b via scaleway, max 112000t

--- > openai/gpt-oss-120b:ovhcloud, openai/gpt-oss-120b via ovhcloud, max 115072t

--- > meta-llama/Llama-3.1-8B-Instruct:cheapest, meta-llama/Llama-3.1-8B-Instruct (cheapest), max 384t

--- > meta-llama/Llama-3.1-8B-Instruct:fastest, meta-llama/Llama-3.1-8B-Instruct (fastest), max 384t

--- > meta-llama/Llama-3.1-8B-Instruct:sambanova, meta-llama/Llama-3.1-8B-Instruct via sambanova, max 384t

--- > meta-llama/Llama-3.1-8B-Instruct:scaleway, meta-llama/Llama-3.1-8B-Instruct via scaleway, max 112000t

--- > meta-llama/Llama-3.1-8B-Instruct:ovhcloud, meta-llama/Llama-3.1-8B-Instruct via ovhcloud, max 115072t

--- > deepseek-ai/DeepSeek-V3.2:cheapest, deepseek-ai/DeepSeek-V3.2 (cheapest), max 147840t

--- > deepseek-ai/DeepSeek-V3.2:fastest, deepseek-ai/DeepSeek-V3.2 (fastest), max 147840t

--- > deepseek-ai/DeepSeek-V3.2:novita, deepseek-ai/DeepSeek-V3.2 via novita, max 147840t

--- > deepseek-ai/DeepSeek-V3.2:fireworks-ai, deepseek-ai/DeepSeek-V3.2 via fireworks-ai, max 147840t

--- > google/gemma-3-27b-it:cheapest, google/gemma-3-27b-it (cheapest), max 112000t

--- > google/gemma-3-27b-it:fastest, google/gemma-3-27b-it (fastest), max 112000t

--- > google/gemma-3-27b-it:scaleway, google/gemma-3-27b-it via scaleway, max 112000t

--- > Qwen/Qwen3-8B:cheapest, Qwen/Qwen3-8B (cheapest), max 24960t

--- > Qwen/Qwen3-8B:fastest, Qwen/Qwen3-8B (fastest), max 24960t

--- > Qwen/Qwen3-8B:nscale, Qwen/Qwen3-8B via nscale, max 24960t

--- > zai-org/GLM-4.7:cheapest, zai-org/GLM-4.7 (cheapest), max 188800t

--- > zai-org/GLM-4.7:fastest, zai-org/GLM-4.7 (fastest), max 188800t

--- > zai-org/GLM-4.7:novita, zai-org/GLM-4.7 via novita, max 188800t

--- > zai-org/GLM-4.7:zai-org, zai-org/GLM-4.7 via zai-org, max 112000t

--- > Qwen/Qwen3-4B-Instruct-2507:cheapest, Qwen/Qwen3-4B-Instruct-2507 (cheapest), max 246144t

--- > Qwen/Qwen3-4B-Instruct-2507:fastest, Qwen/Qwen3-4B-Instruct-2507 (fastest), max 246144t

--- > Qwen/Qwen3-4B-Instruct-2507:nscale, Qwen/Qwen3-4B-Instruct-2507 via nscale, max 246144t

--- > Qwen/Qwen2.5-7B-Instruct:cheapest, Qwen/Qwen2.5-7B-Instruct (cheapest), max 16768t

--- > Qwen/Qwen2.5-7B-Instruct:fastest, Qwen/Qwen2.5-7B-Instruct (fastest), max 16768t

--- > Qwen/Qwen2.5-7B-Instruct:together, Qwen/Qwen2.5-7B-Instruct via together, max 16768t

--- > Qwen/Qwen3-Coder-Next-FP8:cheapest, Qwen/Qwen3-Coder-Next-FP8 (cheapest), max 246144t

--- > Qwen/Qwen3-Coder-Next-FP8:fastest, Qwen/Qwen3-Coder-Next-FP8 (fastest), max 246144t

--- > Qwen/Qwen3-Coder-Next-FP8:together, Qwen/Qwen3-Coder-Next-FP8 via together, max 246144t

--- > deepseek-ai/DeepSeek-R1:cheapest, deepseek-ai/DeepSeek-R1 (cheapest), max 48000t

--- > deepseek-ai/DeepSeek-R1:fastest, deepseek-ai/DeepSeek-R1 (fastest), max 48000t

--- > deepseek-ai/DeepSeek-R1:novita, deepseek-ai/DeepSeek-R1 via novita, max 48000t

--- > deepseek-ai/DeepSeek-R1:sambanova, deepseek-ai/DeepSeek-R1 via sambanova, max 112000t

--- > Qwen/Qwen2.5-Coder-7B-Instruct, Qwen/Qwen2.5-Coder-7B-Instruct, max 115072t

--- > meta-llama/Llama-3.2-3B-Instruct:cheapest, meta-llama/Llama-3.2-3B-Instruct (cheapest), max 115072t

--- > meta-llama/Llama-3.2-3B-Instruct:fastest, meta-llama/Llama-3.2-3B-Instruct (fastest), max 115072t

--- > meta-llama/Llama-3.2-3B-Instruct:hyperbolic, meta-llama/Llama-3.2-3B-Instruct via hyperbolic, max 115072t

--- > meta-llama/Llama-3.2-3B-Instruct:together, meta-llama/Llama-3.2-3B-Instruct via together, max 115072t

--- > Qwen/Qwen3-VL-8B-Instruct:cheapest, Qwen/Qwen3-VL-8B-Instruct (cheapest), max 115072t

--- > Qwen/Qwen3-VL-8B-Instruct:fastest, Qwen/Qwen3-VL-8B-Instruct (fastest), max 115072t

--- > Qwen/Qwen3-VL-8B-Instruct:novita, Qwen/Qwen3-VL-8B-Instruct via novita, max 115072t

--- > Qwen/Qwen3-VL-8B-Instruct:together, Qwen/Qwen3-VL-8B-Instruct via together, max 246144t

--- > meta-llama/Llama-3.3-70B-Instruct:cheapest, meta-llama/Llama-3.3-70B-Instruct (cheapest), max 115072t

--- > meta-llama/Llama-3.3-70B-Instruct:fastest, meta-llama/Llama-3.3-70B-Instruct (fastest), max 115072t

--- > meta-llama/Llama-3.3-70B-Instruct:groq, meta-llama/Llama-3.3-70B-Instruct via groq, max 115072t

--- > meta-llama/Llama-3.3-70B-Instruct:sambanova, meta-llama/Llama-3.3-70B-Instruct via sambanova, max 115072t

--- > meta-llama/Llama-3.3-70B-Instruct:hyperbolic, meta-llama/Llama-3.3-70B-Instruct via hyperbolic, max 115072t

--- > meta-llama/Llama-3.3-70B-Instruct:together, meta-llama/Llama-3.3-70B-Instruct via together, max 115072t

--- > meta-llama/Llama-3.3-70B-Instruct:scaleway, meta-llama/Llama-3.3-70B-Instruct via scaleway, max 112000t

--- > meta-llama/Llama-3.3-70B-Instruct:ovhcloud, meta-llama/Llama-3.3-70B-Instruct via ovhcloud, max 115072t

--- > deepseek-ai/DeepSeek-V3.2-Exp:cheapest, deepseek-ai/DeepSeek-V3.2-Exp (cheapest), max 147840t

--- > deepseek-ai/DeepSeek-V3.2-Exp:fastest, deepseek-ai/DeepSeek-V3.2-Exp (fastest), max 147840t

--- > deepseek-ai/DeepSeek-V3.2-Exp:novita, deepseek-ai/DeepSeek-V3.2-Exp via novita, max 147840t

--- > Qwen/Qwen3-4B-Thinking-2507:cheapest, Qwen/Qwen3-4B-Thinking-2507 (cheapest), max 246144t

--- > Qwen/Qwen3-4B-Thinking-2507:fastest, Qwen/Qwen3-4B-Thinking-2507 (fastest), max 246144t

--- > Qwen/Qwen3-4B-Thinking-2507:nscale, Qwen/Qwen3-4B-Thinking-2507 via nscale, max 246144t

--- > moonshotai/Kimi-K2-Thinking:cheapest, moonshotai/Kimi-K2-Thinking (cheapest), max 246144t

--- > moonshotai/Kimi-K2-Thinking:fastest, moonshotai/Kimi-K2-Thinking (fastest), max 246144t

--- > moonshotai/Kimi-K2-Thinking:novita, moonshotai/Kimi-K2-Thinking via novita, max 246144t

--- > moonshotai/Kimi-K2-Thinking:together, moonshotai/Kimi-K2-Thinking via together, max 246144t

--- > XiaomiMiMo/MiMo-V2-Flash:cheapest, XiaomiMiMo/MiMo-V2-Flash (cheapest), max 246144t

--- > XiaomiMiMo/MiMo-V2-Flash:fastest, XiaomiMiMo/MiMo-V2-Flash (fastest), max 246144t

--- > XiaomiMiMo/MiMo-V2-Flash:novita, XiaomiMiMo/MiMo-V2-Flash via novita, max 246144t

--- > google/gemma-3n-E4B-it, google/gemma-3n-E4B-it, max 16768t

--- > meta-llama/Meta-Llama-3-8B-Instruct, meta-llama/Meta-Llama-3-8B-Instruct, max 1t

--- > Qwen/Qwen3-32B:cheapest, Qwen/Qwen3-32B (cheapest), max 16768t

--- > Qwen/Qwen3-32B:fastest, Qwen/Qwen3-32B (fastest), max 16768t

--- > Qwen/Qwen3-32B:groq, Qwen/Qwen3-32B via groq, max 115072t

--- > Qwen/Qwen3-32B:sambanova, Qwen/Qwen3-32B via sambanova, max 16768t

--- > Qwen/Qwen3-32B:nscale, Qwen/Qwen3-32B via nscale, max 24960t

--- > Qwen/Qwen3-32B:ovhcloud, Qwen/Qwen3-32B via ovhcloud, max 16768t

--- > Qwen/Qwen3-VL-30B-A3B-Instruct:cheapest, Qwen/Qwen3-VL-30B-A3B-Instruct (cheapest), max 115072t

--- > Qwen/Qwen3-VL-30B-A3B-Instruct:fastest, Qwen/Qwen3-VL-30B-A3B-Instruct (fastest), max 115072t

--- > Qwen/Qwen3-VL-30B-A3B-Instruct:novita, Qwen/Qwen3-VL-30B-A3B-Instruct via novita, max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Instruct:cheapest, Qwen/Qwen3-Next-80B-A3B-Instruct (cheapest), max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Instruct:fastest, Qwen/Qwen3-Next-80B-A3B-Instruct (fastest), max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Instruct:novita, Qwen/Qwen3-Next-80B-A3B-Instruct via novita, max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Instruct:hyperbolic, Qwen/Qwen3-Next-80B-A3B-Instruct via hyperbolic, max 246144t

--- > Qwen/Qwen3-Next-80B-A3B-Instruct:together, Qwen/Qwen3-Next-80B-A3B-Instruct via together, max 246144t

--- > deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, max 115072t

--- > mistralai/Mistral-7B-Instruct-v0.2, mistralai/Mistral-7B-Instruct-v0.2, max 16768t

--- > meta-llama/Llama-3.2-1B-Instruct, meta-llama/Llama-3.2-1B-Instruct, max 115000t

--- > Qwen/Qwen3-14B:cheapest, Qwen/Qwen3-14B (cheapest), max 24960t

--- > Qwen/Qwen3-14B:fastest, Qwen/Qwen3-14B (fastest), max 24960t

--- > Qwen/Qwen3-14B:nscale, Qwen/Qwen3-14B via nscale, max 24960t

--- > moonshotai/Kimi-K2-Instruct:cheapest, moonshotai/Kimi-K2-Instruct (cheapest), max 115072t

--- > moonshotai/Kimi-K2-Instruct:fastest, moonshotai/Kimi-K2-Instruct (fastest), max 115072t

--- > moonshotai/Kimi-K2-Instruct:novita, moonshotai/Kimi-K2-Instruct via novita, max 115072t

--- > moonshotai/Kimi-K2-Instruct:together, moonshotai/Kimi-K2-Instruct via together, max 115072t

--- > Qwen/Qwen2.5-Coder-32B-Instruct:cheapest, Qwen/Qwen2.5-Coder-32B-Instruct (cheapest), max 112000t

--- > Qwen/Qwen2.5-Coder-32B-Instruct:fastest, Qwen/Qwen2.5-Coder-32B-Instruct (fastest), max 112000t

--- > Qwen/Qwen2.5-Coder-32B-Instruct:scaleway, Qwen/Qwen2.5-Coder-32B-Instruct via scaleway, max 112000t

--- > MiniMaxAI/MiniMax-M2:cheapest, MiniMaxAI/MiniMax-M2 (cheapest), max 188800t

--- > MiniMaxAI/MiniMax-M2:fastest, MiniMaxAI/MiniMax-M2 (fastest), max 188800t

--- > MiniMaxAI/MiniMax-M2:novita, MiniMaxAI/MiniMax-M2 via novita, max 188800t

--- > nvidia/NVIDIA-Nemotron-Nano-9B-v2:cheapest, nvidia/NVIDIA-Nemotron-Nano-9B-v2 (cheapest), max 115072t

--- > nvidia/NVIDIA-Nemotron-Nano-9B-v2:fastest, nvidia/NVIDIA-Nemotron-Nano-9B-v2 (fastest), max 115072t

--- > nvidia/NVIDIA-Nemotron-Nano-9B-v2:together, nvidia/NVIDIA-Nemotron-Nano-9B-v2 via together, max 115072t

--- > meta-llama/Llama-3.1-70B-Instruct:cheapest, meta-llama/Llama-3.1-70B-Instruct (cheapest), max 112000t

--- > meta-llama/Llama-3.1-70B-Instruct:fastest, meta-llama/Llama-3.1-70B-Instruct (fastest), max 112000t

--- > meta-llama/Llama-3.1-70B-Instruct:scaleway, meta-llama/Llama-3.1-70B-Instruct via scaleway, max 112000t

--- > MiniMaxAI/MiniMax-M2.1:cheapest, MiniMaxAI/MiniMax-M2.1 (cheapest), max 188800t

--- > MiniMaxAI/MiniMax-M2.1:fastest, MiniMaxAI/MiniMax-M2.1 (fastest), max 188800t

--- > MiniMaxAI/MiniMax-M2.1:novita, MiniMaxAI/MiniMax-M2.1 via novita, max 188800t

--- > moonshotai/Kimi-K2-Instruct-0905:cheapest, moonshotai/Kimi-K2-Instruct-0905 (cheapest), max 246144t

--- > moonshotai/Kimi-K2-Instruct-0905:fastest, moonshotai/Kimi-K2-Instruct-0905 (fastest), max 246144t

--- > moonshotai/Kimi-K2-Instruct-0905:groq, moonshotai/Kimi-K2-Instruct-0905 via groq, max 246144t

--- > moonshotai/Kimi-K2-Instruct-0905:novita, moonshotai/Kimi-K2-Instruct-0905 via novita, max 246144t

--- > moonshotai/Kimi-K2-Instruct-0905:together, moonshotai/Kimi-K2-Instruct-0905 via together, max 246144t

--- > Qwen/Qwen3-Coder-30B-A3B-Instruct:cheapest, Qwen/Qwen3-Coder-30B-A3B-Instruct (cheapest), max 246144t

--- > Qwen/Qwen3-Coder-30B-A3B-Instruct:fastest, Qwen/Qwen3-Coder-30B-A3B-Instruct (fastest), max 246144t

--- > Qwen/Qwen3-Coder-30B-A3B-Instruct:scaleway, Qwen/Qwen3-Coder-30B-A3B-Instruct via scaleway, max 112000t

--- > Qwen/Qwen3-Coder-30B-A3B-Instruct:ovhcloud, Qwen/Qwen3-Coder-30B-A3B-Instruct via ovhcloud, max 246144t

--- > openai/gpt-oss-safeguard-20b:cheapest, openai/gpt-oss-safeguard-20b (cheapest), max 115072t

--- > openai/gpt-oss-safeguard-20b:fastest, openai/gpt-oss-safeguard-20b (fastest), max 115072t

--- > openai/gpt-oss-safeguard-20b:groq, openai/gpt-oss-safeguard-20b via groq, max 115072t

--- > Qwen/Qwen2.5-VL-7B-Instruct, Qwen/Qwen2.5-VL-7B-Instruct, max 16768t

--- > ServiceNow-AI/Apriel-1.6-15b-Thinker, ServiceNow-AI/Apriel-1.6-15b-Thinker, max 115072t

--- > allenai/Molmo2-8B, allenai/Molmo2-8B, max 112000t

--- > Qwen/QwQ-32B:cheapest, Qwen/QwQ-32B (cheapest), max 115072t

--- > Qwen/QwQ-32B:fastest, Qwen/QwQ-32B (fastest), max 115072t

--- > Qwen/QwQ-32B:nscale, Qwen/QwQ-32B via nscale, max 115072t

--- > Qwen/Qwen2.5-VL-72B-Instruct, Qwen/Qwen2.5-VL-72B-Instruct, max 16768t

--- > Qwen/Qwen3-VL-235B-A22B-Thinking, Qwen/Qwen3-VL-235B-A22B-Thinking, max 115072t

--- > zai-org/GLM-4.6V-Flash:cheapest, zai-org/GLM-4.6V-Flash (cheapest), max 115072t

--- > zai-org/GLM-4.6V-Flash:fastest, zai-org/GLM-4.6V-Flash (fastest), max 115072t

--- > zai-org/GLM-4.6V-Flash:novita, zai-org/GLM-4.6V-Flash via novita, max 115072t

--- > zai-org/GLM-4.6V-Flash:zai-org, zai-org/GLM-4.6V-Flash via zai-org, max 112000t

--- > Qwen/Qwen3-30B-A3B:cheapest, Qwen/Qwen3-30B-A3B (cheapest), max 24960t

--- > Qwen/Qwen3-30B-A3B:fastest, Qwen/Qwen3-30B-A3B (fastest), max 24960t

--- > Qwen/Qwen3-30B-A3B:novita, Qwen/Qwen3-30B-A3B via novita, max 24960t

--- > Qwen/Qwen3-VL-32B-Instruct:cheapest, Qwen/Qwen3-VL-32B-Instruct (cheapest), max 246144t

--- > Qwen/Qwen3-VL-32B-Instruct:fastest, Qwen/Qwen3-VL-32B-Instruct (fastest), max 246144t

--- > Qwen/Qwen3-VL-32B-Instruct:together, Qwen/Qwen3-VL-32B-Instruct via together, max 246144t

--- > meta-llama/Llama-4-Scout-17B-16E-Instruct:cheapest, meta-llama/Llama-4-Scout-17B-16E-Instruct (cheapest), max 115072t

--- > meta-llama/Llama-4-Scout-17B-16E-Instruct:fastest, meta-llama/Llama-4-Scout-17B-16E-Instruct (fastest), max 115072t

--- > meta-llama/Llama-4-Scout-17B-16E-Instruct:groq, meta-llama/Llama-4-Scout-17B-16E-Instruct via groq, max 115072t

--- > meta-llama/Llama-4-Scout-17B-16E-Instruct:nscale, meta-llama/Llama-4-Scout-17B-16E-Instruct via nscale, max 874000t

--- > Qwen/Qwen3-VL-30B-A3B-Thinking:cheapest, Qwen/Qwen3-VL-30B-A3B-Thinking (cheapest), max 115072t

--- > Qwen/Qwen3-VL-30B-A3B-Thinking:fastest, Qwen/Qwen3-VL-30B-A3B-Thinking (fastest), max 115072t

--- > Qwen/Qwen3-VL-30B-A3B-Thinking:novita, Qwen/Qwen3-VL-30B-A3B-Thinking via novita, max 115072t

--- > HuggingFaceTB/SmolLM3-3B:cheapest, HuggingFaceTB/SmolLM3-3B (cheapest), max 112000t

--- > HuggingFaceTB/SmolLM3-3B:fastest, HuggingFaceTB/SmolLM3-3B (fastest), max 112000t

--- > HuggingFaceTB/SmolLM3-3B:hf-inference, HuggingFaceTB/SmolLM3-3B via hf-inference, max 112000t

--- > deepseek-ai/DeepSeek-V3:cheapest, deepseek-ai/DeepSeek-V3 (cheapest), max 48000t

--- > deepseek-ai/DeepSeek-V3:fastest, deepseek-ai/DeepSeek-V3 (fastest), max 48000t

--- > deepseek-ai/DeepSeek-V3:novita, deepseek-ai/DeepSeek-V3 via novita, max 48000t

--- > deepseek-ai/DeepSeek-V3:together, deepseek-ai/DeepSeek-V3 via together, max 115072t

--- > deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, max 115072t

--- > zai-org/AutoGLM-Phone-9B-Multilingual, zai-org/AutoGLM-Phone-9B-Multilingual, max 49536t

--- > CohereLabs/command-a-reasoning-08-2025:cheapest, CohereLabs/command-a-reasoning-08-2025 (cheapest), max 112000t

--- > CohereLabs/command-a-reasoning-08-2025:fastest, CohereLabs/command-a-reasoning-08-2025 (fastest), max 112000t

--- > CohereLabs/command-a-reasoning-08-2025:cohere, CohereLabs/command-a-reasoning-08-2025 via cohere, max 112000t

--- > allenai/Olmo-3.1-32B-Think, allenai/Olmo-3.1-32B-Think, max 112000t

--- > EssentialAI/rnj-1-instruct:cheapest, EssentialAI/rnj-1-instruct (cheapest), max 16768t

--- > EssentialAI/rnj-1-instruct:fastest, EssentialAI/rnj-1-instruct (fastest), max 16768t

--- > EssentialAI/rnj-1-instruct:together, EssentialAI/rnj-1-instruct via together, max 16768t

--- > deepseek-ai/DeepSeek-R1-0528:cheapest, deepseek-ai/DeepSeek-R1-0528 (cheapest), max 115072t

--- > deepseek-ai/DeepSeek-R1-0528:fastest, deepseek-ai/DeepSeek-R1-0528 (fastest), max 115072t

--- > deepseek-ai/DeepSeek-R1-0528:novita, deepseek-ai/DeepSeek-R1-0528 via novita, max 147840t

--- > deepseek-ai/DeepSeek-R1-0528:sambanova, deepseek-ai/DeepSeek-R1-0528 via sambanova, max 115072t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:cheapest, Qwen/Qwen3-235B-A22B-Instruct-2507 (cheapest), max 16768t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:fastest, Qwen/Qwen3-235B-A22B-Instruct-2507 (fastest), max 16768t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:novita, Qwen/Qwen3-235B-A22B-Instruct-2507 via novita, max 115072t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:nscale, Qwen/Qwen3-235B-A22B-Instruct-2507 via nscale, max 16768t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:hyperbolic, Qwen/Qwen3-235B-A22B-Instruct-2507 via hyperbolic, max 246144t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:together, Qwen/Qwen3-235B-A22B-Instruct-2507 via together, max 246144t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:fireworks-ai, Qwen/Qwen3-235B-A22B-Instruct-2507 via fireworks-ai, max 246144t

--- > Qwen/Qwen3-235B-A22B-Instruct-2507:scaleway, Qwen/Qwen3-235B-A22B-Instruct-2507 via scaleway, max 112000t

--- > CohereLabs/aya-vision-32b, CohereLabs/aya-vision-32b, max 112000t

--- > zai-org/GLM-4.6V:cheapest, zai-org/GLM-4.6V (cheapest), max 112000t

--- > zai-org/GLM-4.6V:fastest, zai-org/GLM-4.6V (fastest), max 112000t

--- > zai-org/GLM-4.6V:zai-org, zai-org/GLM-4.6V via zai-org, max 112000t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8:cheapest, meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 (cheapest), max 1032576t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8:fastest, meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 (fastest), max 1032576t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8:together, meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 via together, max 1032576t

--- > Qwen/Qwen2.5-72B-Instruct:cheapest, Qwen/Qwen2.5-72B-Instruct (cheapest), max 16000t

--- > Qwen/Qwen2.5-72B-Instruct:fastest, Qwen/Qwen2.5-72B-Instruct (fastest), max 16000t

--- > Qwen/Qwen2.5-72B-Instruct:novita, Qwen/Qwen2.5-72B-Instruct via novita, max 16000t

--- > deepseek-ai/DeepSeek-Prover-V2-671B, deepseek-ai/DeepSeek-Prover-V2-671B, max 144000t

--- > deepseek-ai/DeepSeek-V3.1-Terminus:cheapest, deepseek-ai/DeepSeek-V3.1-Terminus (cheapest), max 115072t

--- > deepseek-ai/DeepSeek-V3.1-Terminus:fastest, deepseek-ai/DeepSeek-V3.1-Terminus (fastest), max 115072t

--- > deepseek-ai/DeepSeek-V3.1-Terminus:novita, deepseek-ai/DeepSeek-V3.1-Terminus via novita, max 115072t

--- > deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, max 115072t

--- > Qwen/Qwen3-VL-235B-A22B-Instruct:cheapest, Qwen/Qwen3-VL-235B-A22B-Instruct (cheapest), max 115072t

--- > Qwen/Qwen3-VL-235B-A22B-Instruct:fastest, Qwen/Qwen3-VL-235B-A22B-Instruct (fastest), max 115072t

--- > Qwen/Qwen3-VL-235B-A22B-Instruct:novita, Qwen/Qwen3-VL-235B-A22B-Instruct via novita, max 115072t

--- > zai-org/GLM-4.5-Air-FP8:cheapest, zai-org/GLM-4.5-Air-FP8 (cheapest), max 115072t

--- > zai-org/GLM-4.5-Air-FP8:fastest, zai-org/GLM-4.5-Air-FP8 (fastest), max 115072t

--- > zai-org/GLM-4.5-Air-FP8:together, zai-org/GLM-4.5-Air-FP8 via together, max 115072t

--- > meta-llama/Meta-Llama-3-70B-Instruct, meta-llama/Meta-Llama-3-70B-Instruct, max 1t

--- > CohereLabs/c4ai-command-r7b-12-2024:cheapest, CohereLabs/c4ai-command-r7b-12-2024 (cheapest), max 112000t

--- > CohereLabs/c4ai-command-r7b-12-2024:fastest, CohereLabs/c4ai-command-r7b-12-2024 (fastest), max 112000t

--- > CohereLabs/c4ai-command-r7b-12-2024:cohere, CohereLabs/c4ai-command-r7b-12-2024 via cohere, max 112000t

--- > swiss-ai/Apertus-8B-Instruct-2509, swiss-ai/Apertus-8B-Instruct-2509, max 112000t

--- > zai-org/GLM-4.6:cheapest, zai-org/GLM-4.6 (cheapest), max 186752t

--- > zai-org/GLM-4.6:fastest, zai-org/GLM-4.6 (fastest), max 186752t

--- > zai-org/GLM-4.6:novita, zai-org/GLM-4.6 via novita, max 188800t

--- > zai-org/GLM-4.6:cerebras, zai-org/GLM-4.6 via cerebras, max 112000t

--- > zai-org/GLM-4.6:together, zai-org/GLM-4.6 via together, max 186752t

--- > zai-org/GLM-4.6:zai-org, zai-org/GLM-4.6 via zai-org, max 112000t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8:cheapest, Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 (cheapest), max 246144t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8:fastest, Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 (fastest), max 246144t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8:together, Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 via together, max 246144t

--- > zai-org/GLM-4-32B-0414, zai-org/GLM-4-32B-0414, max 16000t

--- > CohereLabs/c4ai-command-a-03-2025:cheapest, CohereLabs/c4ai-command-a-03-2025 (cheapest), max 112000t

--- > CohereLabs/c4ai-command-a-03-2025:fastest, CohereLabs/c4ai-command-a-03-2025 (fastest), max 112000t

--- > CohereLabs/c4ai-command-a-03-2025:cohere, CohereLabs/c4ai-command-a-03-2025 via cohere, max 112000t

--- > utter-project/EuroLLM-22B-Instruct-2512, utter-project/EuroLLM-22B-Instruct-2512, max 112000t

--- > deepseek-ai/DeepSeek-V3.1:cheapest, deepseek-ai/DeepSeek-V3.1 (cheapest), max 115072t

--- > deepseek-ai/DeepSeek-V3.1:fastest, deepseek-ai/DeepSeek-V3.1 (fastest), max 115072t

--- > deepseek-ai/DeepSeek-V3.1:novita, deepseek-ai/DeepSeek-V3.1 via novita, max 115072t

--- > deepseek-ai/DeepSeek-V3.1:together, deepseek-ai/DeepSeek-V3.1 via together, max 115072t

--- > deepseek-ai/DeepSeek-V3.1:fireworks-ai, deepseek-ai/DeepSeek-V3.1 via fireworks-ai, max 147840t

--- > MiniMaxAI/MiniMax-M1-80k:cheapest, MiniMaxAI/MiniMax-M1-80k (cheapest), max 984000t

--- > MiniMaxAI/MiniMax-M1-80k:fastest, MiniMaxAI/MiniMax-M1-80k (fastest), max 984000t

--- > MiniMaxAI/MiniMax-M1-80k:novita, MiniMaxAI/MiniMax-M1-80k via novita, max 984000t

--- > CohereLabs/command-a-translate-08-2025:cheapest, CohereLabs/command-a-translate-08-2025 (cheapest), max 112000t

--- > CohereLabs/command-a-translate-08-2025:fastest, CohereLabs/command-a-translate-08-2025 (fastest), max 112000t

--- > CohereLabs/command-a-translate-08-2025:cohere, CohereLabs/command-a-translate-08-2025 via cohere, max 112000t

--- > allenai/Olmo-3.1-32B-Instruct:cheapest, allenai/Olmo-3.1-32B-Instruct (cheapest), max 112000t

--- > allenai/Olmo-3.1-32B-Instruct:fastest, allenai/Olmo-3.1-32B-Instruct (fastest), max 112000t

--- > allenai/Olmo-3.1-32B-Instruct:publicai, allenai/Olmo-3.1-32B-Instruct via publicai, max 112000t

--- > Qwen/Qwen3-Next-80B-A3B-Thinking:cheapest, Qwen/Qwen3-Next-80B-A3B-Thinking (cheapest), max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Thinking:fastest, Qwen/Qwen3-Next-80B-A3B-Thinking (fastest), max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Thinking:novita, Qwen/Qwen3-Next-80B-A3B-Thinking via novita, max 115072t

--- > Qwen/Qwen3-Next-80B-A3B-Thinking:hyperbolic, Qwen/Qwen3-Next-80B-A3B-Thinking via hyperbolic, max 246144t

--- > Qwen/Qwen3-Next-80B-A3B-Thinking:together, Qwen/Qwen3-Next-80B-A3B-Thinking via together, max 246144t

--- > deepseek-ai/DeepSeek-V3-0324:cheapest, deepseek-ai/DeepSeek-V3-0324 (cheapest), max 115072t

--- > deepseek-ai/DeepSeek-V3-0324:fastest, deepseek-ai/DeepSeek-V3-0324 (fastest), max 115072t

--- > deepseek-ai/DeepSeek-V3-0324:novita, deepseek-ai/DeepSeek-V3-0324 via novita, max 147840t

--- > deepseek-ai/DeepSeek-V3-0324:sambanova, deepseek-ai/DeepSeek-V3-0324 via sambanova, max 115072t

--- > deepseek-ai/DeepSeek-V3-0324:hyperbolic, deepseek-ai/DeepSeek-V3-0324 via hyperbolic, max 147840t

--- > deepseek-ai/DeepSeek-V3-0324:together, deepseek-ai/DeepSeek-V3-0324 via together, max 115072t

--- > meta-llama/Llama-Guard-4-12B, meta-llama/Llama-Guard-4-12B, max 115072t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct:cheapest, Qwen/Qwen3-Coder-480B-A35B-Instruct (cheapest), max 246144t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct:fastest, Qwen/Qwen3-Coder-480B-A35B-Instruct (fastest), max 246144t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct:novita, Qwen/Qwen3-Coder-480B-A35B-Instruct via novita, max 246144t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct:cerebras, Qwen/Qwen3-Coder-480B-A35B-Instruct via cerebras, max 112000t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct:hyperbolic, Qwen/Qwen3-Coder-480B-A35B-Instruct via hyperbolic, max 246144t

--- > Qwen/Qwen3-Coder-480B-A35B-Instruct:together, Qwen/Qwen3-Coder-480B-A35B-Instruct via together, max 246144t

--- > Qwen/Qwen3-235B-A22B-Thinking-2507:cheapest, Qwen/Qwen3-235B-A22B-Thinking-2507 (cheapest), max 115072t

--- > Qwen/Qwen3-235B-A22B-Thinking-2507:fastest, Qwen/Qwen3-235B-A22B-Thinking-2507 (fastest), max 115072t

--- > Qwen/Qwen3-235B-A22B-Thinking-2507:novita, Qwen/Qwen3-235B-A22B-Thinking-2507 via novita, max 115072t

--- > Qwen/Qwen3-235B-A22B-Thinking-2507:fireworks-ai, Qwen/Qwen3-235B-A22B-Thinking-2507 via fireworks-ai, max 246144t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct:cheapest, meta-llama/Llama-4-Maverick-17B-128E-Instruct (cheapest), max 115072t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct:fastest, meta-llama/Llama-4-Maverick-17B-128E-Instruct (fastest), max 115072t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct:groq, meta-llama/Llama-4-Maverick-17B-128E-Instruct via groq, max 115072t

--- > meta-llama/Llama-4-Maverick-17B-128E-Instruct:sambanova, meta-llama/Llama-4-Maverick-17B-128E-Instruct via sambanova, max 115072t

--- > Qwen/Qwen2.5-VL-32B-Instruct, Qwen/Qwen2.5-VL-32B-Instruct, max 112000t

--- > CohereLabs/command-a-vision-07-2025, CohereLabs/command-a-vision-07-2025, max 112000t

--- > katanemo/Arch-Router-1.5B, katanemo/Arch-Router-1.5B, max 112000t

--- > deepcogito/cogito-671b-v2.1:cheapest, deepcogito/cogito-671b-v2.1 (cheapest), max 147840t

--- > deepcogito/cogito-671b-v2.1:fastest, deepcogito/cogito-671b-v2.1 (fastest), max 147840t

--- > deepcogito/cogito-671b-v2.1:fireworks-ai, deepcogito/cogito-671b-v2.1 via fireworks-ai, max 147840t

--- > Qwen/Qwen2.5-Coder-3B-Instruct, Qwen/Qwen2.5-Coder-3B-Instruct, max 16768t

--- > tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4, tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4, max 115072t

--- > baidu/ERNIE-4.5-VL-424B-A47B-Base-PT, baidu/ERNIE-4.5-VL-424B-A47B-Base-PT, max 107000t

--- > zai-org/GLM-4.5:cheapest, zai-org/GLM-4.5 (cheapest), max 115072t

--- > zai-org/GLM-4.5:fastest, zai-org/GLM-4.5 (fastest), max 115072t

--- > zai-org/GLM-4.5:novita, zai-org/GLM-4.5 via novita, max 115072t

--- > zai-org/GLM-4.5:zai-org, zai-org/GLM-4.5 via zai-org, max 112000t

--- > deepseek-ai/DeepSeek-R1-Distill-Llama-8B, deepseek-ai/DeepSeek-R1-Distill-Llama-8B, max 115072t

--- > aisingapore/Gemma-SEA-LION-v4-27B-IT, aisingapore/Gemma-SEA-LION-v4-27B-IT, max 112000t

--- > baidu/ERNIE-4.5-VL-28B-A3B-PT, baidu/ERNIE-4.5-VL-28B-A3B-PT, max 14000t

--- > zai-org/GLM-4.5-Air:cheapest, zai-org/GLM-4.5-Air (cheapest), max 115072t

--- > zai-org/GLM-4.5-Air:fastest, zai-org/GLM-4.5-Air (fastest), max 115072t

--- > zai-org/GLM-4.5-Air:novita, zai-org/GLM-4.5-Air via novita, max 115072t

--- > zai-org/GLM-4.5-Air:zai-org, zai-org/GLM-4.5-Air via zai-org, max 112000t

--- > Qwen/Qwen3-235B-A22B:cheapest, Qwen/Qwen3-235B-A22B (cheapest), max 16000t

--- > Qwen/Qwen3-235B-A22B:fastest, Qwen/Qwen3-235B-A22B (fastest), max 16000t

--- > Qwen/Qwen3-235B-A22B:nscale, Qwen/Qwen3-235B-A22B via nscale, max 16000t

--- > swiss-ai/Apertus-70B-Instruct-2509, swiss-ai/Apertus-70B-Instruct-2509, max 112000t

--- > zai-org/GLM-4.5V:cheapest, zai-org/GLM-4.5V (cheapest), max 49536t

--- > zai-org/GLM-4.5V:fastest, zai-org/GLM-4.5V (fastest), max 49536t

--- > zai-org/GLM-4.5V:novita, zai-org/GLM-4.5V via novita, max 49536t

--- > zai-org/GLM-4.5V:zai-org, zai-org/GLM-4.5V via zai-org, max 112000t

--- > zai-org/GLM-4.6-FP8:cheapest, zai-org/GLM-4.6-FP8 (cheapest), max 112000t

--- > zai-org/GLM-4.6-FP8:fastest, zai-org/GLM-4.6-FP8 (fastest), max 112000t

--- > zai-org/GLM-4.6-FP8:zai-org, zai-org/GLM-4.6-FP8 via zai-org, max 112000t

--- > marin-community/marin-8b-instruct, marin-community/marin-8b-instruct, max 1t

--- > deepseek-ai/DeepSeek-R1-Distill-Llama-70B, deepseek-ai/DeepSeek-R1-Distill-Llama-70B, max 1t

--- > aisingapore/Qwen-SEA-LION-v4-32B-IT:cheapest, aisingapore/Qwen-SEA-LION-v4-32B-IT (cheapest), max 112000t

--- > aisingapore/Qwen-SEA-LION-v4-32B-IT:fastest, aisingapore/Qwen-SEA-LION-v4-32B-IT (fastest), max 112000t

--- > aisingapore/Qwen-SEA-LION-v4-32B-IT:publicai, aisingapore/Qwen-SEA-LION-v4-32B-IT via publicai, max 112000t

--- > zai-org/GLM-4.7-FP8:cheapest, zai-org/GLM-4.7-FP8 (cheapest), max 186752t

--- > zai-org/GLM-4.7-FP8:fastest, zai-org/GLM-4.7-FP8 (fastest), max 186752t

--- > zai-org/GLM-4.7-FP8:together, zai-org/GLM-4.7-FP8 via together, max 186752t

--- > zai-org/GLM-4.7-FP8:zai-org, zai-org/GLM-4.7-FP8 via zai-org, max 112000t

--- > deepcogito/cogito-671b-v2.1-FP8, deepcogito/cogito-671b-v2.1-FP8, max 147840t

--- > allenai/Olmo-3-7B-Think, allenai/Olmo-3-7B-Think, max 112000t

--- > alpindale/WizardLM-2-8x22B, alpindale/WizardLM-2-8x22B, max 49535t

--- > arcee-ai/Trinity-Mini:cheapest, arcee-ai/Trinity-Mini (cheapest), max 112000t

--- > arcee-ai/Trinity-Mini:fastest, arcee-ai/Trinity-Mini (fastest), max 112000t

--- > arcee-ai/Trinity-Mini:together, arcee-ai/Trinity-Mini via together, max 112000t

--- > allenai/Olmo-3-7B-Instruct:cheapest, allenai/Olmo-3-7B-Instruct (cheapest), max 112000t

--- > allenai/Olmo-3-7B-Instruct:fastest, allenai/Olmo-3-7B-Instruct (fastest), max 112000t

--- > allenai/Olmo-3-7B-Instruct:publicai, allenai/Olmo-3-7B-Instruct via publicai, max 112000t

--- > NousResearch/Hermes-2-Pro-Llama-3-8B, NousResearch/Hermes-2-Pro-Llama-3-8B, max 1t

--- > CohereLabs/c4ai-command-r-08-2024:cheapest, CohereLabs/c4ai-command-r-08-2024 (cheapest), max 112000t

--- > CohereLabs/c4ai-command-r-08-2024:fastest, CohereLabs/c4ai-command-r-08-2024 (fastest), max 112000t

--- > CohereLabs/c4ai-command-r-08-2024:cohere, CohereLabs/c4ai-command-r-08-2024 via cohere, max 112000t

--- > dicta-il/DictaLM-3.0-24B-Thinking:cheapest, dicta-il/DictaLM-3.0-24B-Thinking (cheapest), max 112000t

--- > dicta-il/DictaLM-3.0-24B-Thinking:fastest, dicta-il/DictaLM-3.0-24B-Thinking (fastest), max 112000t

--- > dicta-il/DictaLM-3.0-24B-Thinking:publicai, dicta-il/DictaLM-3.0-24B-Thinking via publicai, max 112000t

--- > Sao10K/L3-8B-Stheno-v3.2, Sao10K/L3-8B-Stheno-v3.2, max 1t

--- > zai-org/GLM-4.5V-FP8:cheapest, zai-org/GLM-4.5V-FP8 (cheapest), max 112000t

--- > zai-org/GLM-4.5V-FP8:fastest, zai-org/GLM-4.5V-FP8 (fastest), max 112000t

--- > zai-org/GLM-4.5V-FP8:zai-org, zai-org/GLM-4.5V-FP8 via zai-org, max 112000t

--- > CohereLabs/aya-expanse-32b, CohereLabs/aya-expanse-32b, max 112000t

--- > Sao10K/L3-70B-Euryale-v2.1, Sao10K/L3-70B-Euryale-v2.1, max 1t

--- > zai-org/GLM-4.6V-FP8:cheapest, zai-org/GLM-4.6V-FP8 (cheapest), max 112000t

--- > zai-org/GLM-4.6V-FP8:fastest, zai-org/GLM-4.6V-FP8 (fastest), max 112000t

--- > zai-org/GLM-4.6V-FP8:zai-org, zai-org/GLM-4.6V-FP8 via zai-org, max 112000t

--- > CohereLabs/c4ai-command-r7b-arabic-02-2025:cheapest, CohereLabs/c4ai-command-r7b-arabic-02-2025 (cheapest), max 112000t

--- > CohereLabs/c4ai-command-r7b-arabic-02-2025:fastest, CohereLabs/c4ai-command-r7b-arabic-02-2025 (fastest), max 112000t

--- > CohereLabs/c4ai-command-r7b-arabic-02-2025:cohere, CohereLabs/c4ai-command-r7b-arabic-02-2025 via cohere, max 112000t

--- > Sao10K/L3-8B-Lunaris-v1, Sao10K/L3-8B-Lunaris-v1, max 1t

--- > baidu/ERNIE-4.5-300B-A47B-Base-PT, baidu/ERNIE-4.5-300B-A47B-Base-PT, max 107000t

--- > baidu/ERNIE-4.5-21B-A3B-PT, baidu/ERNIE-4.5-21B-A3B-PT, max 104000t